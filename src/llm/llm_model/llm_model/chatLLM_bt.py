#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# flake8: noqa
#
# Copyright 2025 HuoHaiJie @NXROBO Robotics
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Author:  HuoHaiJie @NXROBO Robotics

# ROS related
import ast
import rclpy
from rclpy.node import Node
from llm_interfaces.srv import ChatLLM,BehavioursTree,ChildTree
from std_msgs.msg import String

# LLM related
import json
import os
import time
from http import HTTPStatus
from random import randint
import re
from llm_config.user_config import UserConfig
import requests


# Global Initialization
config = UserConfig()
# dashscope.api_key = config.dashscope_api_key

class ChatLLMNode(Node):
    def __init__(self):
        super().__init__("ChatLLM_node")
        # åˆ›å»ºè¡Œä¸ºæ ‘çš„æœåŠ¡ç«¯
        self.srv = self.create_service(BehavioursTree, "/llm_input", self.llm_srv_callback)
        self.add_child_tree = self.create_client(ChildTree,"/add_child_tree")

    def handle_dify_stream_response(self,response):
        '''
        å¤„ç†difyæœåŠ¡è¿”å›çš„æµæ•°æ®
        '''
        full_text = ""  # ç´¯ç§¯å®Œæ•´å›å¤
        metadata = None  # å­˜å‚¨å…ƒæ•°æ®
        
        if response.status_code == 200:
            for line in response.iter_lines():
                if line:
                    # è¾“å‡ºåŸå§‹æ•°æ®
                    self.get_logger().info(f"åŸå§‹æ•°æ®: {line}")
                    decoded_line = line.decode('utf-8').strip() # è§£ç å¹¶ç§»é™¤é¦–å°¾ç©ºæ ¼
                    
                    # ä»…å¤„ç†æœ‰æ•ˆäº‹ä»¶è¡Œ
                    if decoded_line.startswith('data:'):
                        try:
                            event_data = json.loads(decoded_line[5:])  # ç§»é™¤"data:"å‰ç¼€
                            event_type = event_data.get("event")
                          
                            # å¤„ç†æ–‡æœ¬åˆ†å—
                            if event_type == "message":
                                chunk = event_data.get("answer", "")
                                full_text += chunk
                                # self.get_logger().info(f"æ”¶åˆ°åˆ†å—: {chunk}")
                            # elif event_type == "workflow_finished":
                            #     top_data = event_data.get("data", {})
                            #     output = top_data.get("outputs", {})
                            #     chunk = output.get("answer", "")
                            #     full_text += chunk
                                # self.get_logger().info(f"æ”¶åˆ°åˆ†å—: {chunk}")
                            elif event_type == "agent_message":
                                chunk = event_data.get("answer", "")
                                full_text += chunk
                                # self.get_logger().info(f"æ”¶åˆ°åˆ†å—: {chunk}")
                            
                            # æ•è·ç»“æŸäº‹ä»¶åŠå…ƒæ•°æ®
                            # elif event_type == "message_end":
                            #     metadata = event_data.get("metadata", {})
                            #     self.get_logger().info(f"æœ€ç»ˆå…ƒæ•°æ®: {metadata}")
                        
                        except json.JSONDecodeError:
                            self.get_logger().error(f"JSONè§£æå¤±è´¥: {decoded_line}")
        else:
            self.get_logger().error(f"è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
            return False

        # è¿”å›å®Œæ•´ç»“æœ
        return {
            "content": full_text,
            # "metadata": metadata
        }

    def upload_photo(self,photo_path,dify_key):
        '''
        ä¸Šä¼ ç…§ç‰‡åˆ°difyæœåŠ¡
        '''
        url = "http://192.168.100.244/v1/files/upload"
        headers = {
            "Authorization": dify_key  # è®¤è¯ä¿¡æ¯
        }
        # 1. ä»¥äºŒè¿›åˆ¶æ¨¡å¼æ‰“å¼€æ–‡ä»¶ï¼Œå¹¶æŒ‡å®š MIME ç±»å‹
        with open(photo_path, 'rb') as file:
            # 2. æ„å»º files å­—å…¸ï¼šé”®åå¿…é¡»ä¸º 'file'ï¼Œå€¼åŒ…å«ï¼ˆæ–‡ä»¶å, æ–‡ä»¶å¯¹è±¡, MIMEç±»å‹ï¼‰
            files = {'file': ('image', file, 'image/png')}  # MIME ç±»å‹éœ€åŒ¹é…æ–‡ä»¶æ ¼å¼[4,6](@ref)
            
            # 3. éæ–‡ä»¶å‚æ•°ï¼ˆå¦‚ userï¼‰é€šè¿‡ data ä¼ é€’
            data = {'user': 'leo'}
            
            # 4. å‘é€ POST è¯·æ±‚ï¼ˆç§»é™¤ json å’Œ stream å‚æ•°ï¼‰
            response = requests.post(url, headers=headers, files=files, data=data)

        photo_id = response.json()["id"]
        self.get_logger().info(f"å›¾ç‰‡ä¸Šä¼ id: {photo_id}")

        return photo_id

    def generate_llm_response(self, messages_input):
        '''
        æ ¹æ®æä¾›çš„è¾“å…¥æ¶ˆæ¯ç”Ÿæˆæ€ç»´é“¾çš„å“åº”ã€‚
        '''
        
        # è°ƒç”¨difyæœåŠ¡-æ€ç»´é“¾æœåŠ¡
        dify_key = "Bearer app-in7itSS6MVPXDpZNTzHQeT5j"  # æ€ç»´é“¾
        url = "http://192.168.100.244/v1/chat-messages" 
        headers = {
            "Authorization": dify_key,  
            "Content-Type": "application/json"  
        }
        data = {
            "inputs": {},
            "query": str(messages_input),
            "response_mode":"streaming",
            "user":"leo",
        }

        # è¯·æ±‚difyæœåŠ¡
        response = requests.post(url, headers=headers,json=data,stream=True)
        
        # é’ˆå¯¹streamingæ ¼å¼å¤„ç†
        result = self.handle_dify_stream_response(response)

        if result:
            # self.get_logger().info(f"\033[36m Qwenå“åº” : \n{result['content']}\033[0m")
            return result['content']
        else:
            return None

    def get_response_classification(self, llm_response):
        '''
        ä»ChatLLMçš„å“åº”ä¸­æå–å“åº”ä¿¡æ¯ã€‚
        return:
        response_type: å“åº”ç±»å‹
        response_data: å“åº”å†…å®¹
        response_description: å“åº”æè¿°
        ''' 

        # 1. æå–å“åº”å†…å®¹ï¼Œåˆå§‹åŒ–æ ‡ç­¾
        chunk = llm_response 
        # åˆå§‹åŒ–æ¶ˆæ¯æ–‡æœ¬ï¼Œé»˜è®¤ä¸ºNone
        response_content = None; response_type = None ; response_data = None ; response_description = None
        self.get_logger().info(f"\033[36m åŸå§‹å“åº”: {chunk}\033[0m")

        # 2.1  å¤„ç†åŒ…å« <think> æ ‡ç­¾çš„æƒ…å†µ
        start_tag = "<think>"
        end_tag = "</think>"
        if start_tag in chunk and end_tag in chunk:
            chunk = re.sub(r'<think>.*?</think>', '', chunk, flags=re.DOTALL).strip()
        
        if not chunk.strip().startswith('{'):
            chunk = '{' + chunk + '}'

        try:
            # 2.2 è§£æJSONå“åº”,æå–å“åº”ç±»å‹ã€å“åº”å†…å®¹ã€å“åº”æè¿°
            response_content = json.loads(chunk)
            response_type = response_content["response_type"]
            response_data = response_content["response_data"]
            response_description = response_content["response_description"]
            self.get_logger().info(f"\033[36m æ£€æµ‹åˆ°å“åº”ç±»å‹: {response_type}\033[0m")
            self.get_logger().info(f"\033[36m æ£€æµ‹åˆ°å“åº”æ•°æ®: {response_data}\033[0m")
            self.get_logger().info(f"\033[36m æ£€æµ‹åˆ°å“åº”æè¿°: {response_description}\033[0m")
        except (KeyError, json.JSONDecodeError) as e:
            self.get_logger().error(f"å“åº”è§£æå¤±è´¥: {str(e)}")


        return response_type, response_data, response_description

    def handle_info(self,info):
        '''
        å¤„ç†infoç±»å‹å“åº”
        '''
        request = ChildTree.Request()
        request.server_name = "audio_output"
        request.server_type = "BehavioursTree"
        request.server_parameters = info
        self.add_child_tree.call_async(request)

    def handle_task_list(self,task_list):
        '''
        å¤„ç†ä»»åŠ¡åˆ—è¡¨
        '''
        server_name = None ; server_type = None ; server_parameters = None
        self.get_logger().info(f"ğŸ“å¼€å§‹å‘å¸ƒä»»åŠ¡åˆ—è¡¨")
        for task in task_list:
            server_name = task["server_name"]
            server_type = task["server_type"]
            server_parameters = task["server_parameters"]

            request = ChildTree.Request()
            request.server_name = server_name
            request.server_type = server_type
            request.server_parameters = server_parameters
            self.add_child_tree.call_async(request)
            
            # self.get_logger().info(f"ä»»åŠ¡: {server_name} {server_type} {server_parameters}")

    def llm_srv_callback(self, request, response):
        '''
        llm_srv_callbackå‡½æ•°åœ¨ChatLLMæœåŠ¡è¢«è°ƒç”¨æ—¶è§¦å‘ã€‚
        è¯¥å‡½æ•°æ˜¯ChatLLMèŠ‚ç‚¹çš„ä¸»è¦åŠŸèƒ½,ç”¨äºå¤„ç†è¾“å…¥æ¶ˆæ¯å¹¶ç”ŸæˆLLMå“åº”ã€‚
        '''
        try:
            # 1. æ˜¾ç¤ºæ”¶åˆ°çš„è¾“å…¥æ¶ˆæ¯
            self.get_logger().info(f"\033[32m æ”¶åˆ°çš„è¾“å…¥æ¶ˆæ¯: {request.input_data}\033[0m")

            # 2. å°†ç”¨æˆ·æ¶ˆæ¯æ·»åŠ åˆ°èŠå¤©å†å²è®°å½•ä¸­,æ ¹æ®å¯¹è¯å†å²ç”ŸæˆèŠå¤©å›å¤
            user_prompt = request.input_data

            # 3. ç”ŸæˆLLMå“åº”
            llm_response = self.generate_llm_response(user_prompt)

            if llm_response: # æ£€æŸ¥æ˜¯å¦ç”Ÿæˆäº†å“åº”
                # 4. æå–å“åº”çš„ç›¸å…³ä¿¡æ¯ (å“åº”ç±»å‹ã€å“åº”å†…å®¹ã€å“åº”æè¿°)
                response_type, response_data, response_description = self.get_response_classification(llm_response)
                # 5. æ ¹æ®å“åº”ç±»å‹åˆ†ç±»å¤„ç†
                if response_type == "info":
                    self.get_logger().info(f"INFOå¤„ç†å®Œæˆ: {response_description}...")
                    self.handle_info(response_description)
                elif response_type == "demo":
                    self.get_logger().info(f"DEMOæ‰§è¡Œ: {response_description}...")
                elif response_type == "task_list":
                    self.get_logger().info(f"TASK LIST: {response_description}...")
                    self.handle_task_list(response_data)
                else:
                    self.get_logger().error(f"æœªçŸ¥å“åº”ç±»å‹: {response_type}")
                
                # è¿”å›å“åº”
                response.success = True
                response.output_data = str(response_description)
                self.get_logger().info(f"LLM processing success: {response_description}")
                return response
            else:
                response.success = False
                return response
        except Exception as e:
            self.get_logger().error(f"Service call exception: {str(e)}")
            response.success = False
            return response


def main(args=None):
    rclpy.init(args=args)
    ChatLLM = ChatLLMNode()
    rclpy.spin(ChatLLM)
    rclpy.shutdown()


if __name__ == "__main__":
    main()

